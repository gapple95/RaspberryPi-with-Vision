{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01. Segmentation\n",
    "\n",
    "## 01-2. Segmentation 개념 및 적용\n",
    "### Segmentation이란?\n",
    "Segmentation(분할)은 이미지 내의 개별 객체나 영역을 구분하는 과정입니다. 대표적으로 다음과 같은 방식이 있습니다.\n",
    "- **Semantic Segmentation**: 같은 클래스의 픽셀을 그룹화 (예: 하늘, 도로, 사람 등)\n",
    "- **Instance Segmentation**: 개별 객체를 분리 (예: 각각의 사람, 자동차 구분)\n",
    "\n",
    "### YOLO-Seg vs. 기존 Segmentation 기법\n",
    "| 기법 | 설명 | 장점 | 단점 |\n",
    "|------|------|------|------|\n",
    "| U-Net | CNN 기반 의료영상, 일반 이미지 분할에 활용 | 높은 정확도, 경량화 가능 | 비교적 속도가 느림 |\n",
    "| DeepLabV3+ | Atrous convolution 활용, 고해상도 이미지 분할 가능 | 높은 정확도 | 모델 크기가 크고 연산량이 많음 |\n",
    "| YOLO-Seg | YOLO 기반 실시간 객체 분할 | 빠른 속도, 객체 감지와 분할 동시 수행 | 기존 Segmentation보다 세밀한 영역 구분이 약할 수 있음 |\n",
    "\n",
    "### 라즈베리파이에서 사용할 수 있는 Segmentation 모델\n",
    "- **TensorFlow Lite(TFLite) 기반 모델**: MobileNet+DeepLabV3\n",
    "- **YOLO-Seg TFLite 변환 후 사용**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02. [실습 8] TFLite 기반 YOLO-Seg 실행\n",
    "1. 라즈베리파이에서 YOLO-Seg 모델 다운로드 및 변환\n",
    "2. OpenCV 및 TFLite로 Segmentation 실행\n",
    "3. 결과 시각화 및 성능 측정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02-1. YOLOv5n을 활용한 실시간 Segmentation\n",
    "\n",
    "### YOLOv5 git 다운로드 및 라이브러리 설치\n",
    "```bash\n",
    "# YOLOv5 저장소 클론\n",
    "git clone https://github.com/ultralytics/yolov5.git\n",
    "cd yolov5\n",
    "\n",
    "# 의존성 패키지 설치 (라즈베리파이에 필요한 라이브러리 포함)\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "### yolov5n-seg 모델 다운로드\n",
    "```bash\n",
    "mkdir weights\n",
    "wget -O weights/yolov5n-seg.pt https://github.com/ultralytics/yolov5/releases/download/v6.2/yolov5n-seg.pt\n",
    "```\n",
    "\n",
    "### YOLOv5s 모델을 tflite 모델로 변환\n",
    "```bash\n",
    "python3 export.py --weights weights/yolov5n-seg.pt \\\n",
    "                 --include tflite \\\n",
    "                 --img 640 \\\n",
    "                 --device cpu\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOLO-seg를 실행하기 위한 라이브러리 설치\n",
    "```bash\n",
    "pip install numpy opencv-python tflite-runtime\n",
    "```\n",
    "\n",
    "### YOLO-seg 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-17 15:19:05.721703: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1739773145.731160   24149 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1739773145.733779   24149 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow.lite as tflite\n",
    "\n",
    "# YOLOv5n-seg TFLite 모델 로드\n",
    "interpreter = tflite.Interpreter(model_path=\"./yolov5/yolov5n-seg-fp16.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# 입력 및 출력 텐서 정보 가져오기\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# 정확도를 높이기 위해 Threshold 조정\n",
    "THRESHOLD = 0.5  \n",
    "\n",
    "# 투명도 조절 (0.0 ~ 1.0)\n",
    "ALPHA = 0.5  \n",
    "\n",
    "# YOLO 모델 해상도 설정 (원본 비율 유지 X, YOLO 학습 크기 유지)\n",
    "YOLO_SIZE = 640  \n",
    "\n",
    "# 이미지 전처리 함수 (YOLO 모델이 학습한 방식과 동일하게 적용)\n",
    "def preprocess_image(image_path, input_shape):\n",
    "    img = cv2.imread(image_path)\n",
    "\n",
    "    # YOLO 입력 크기(640x640)로 강제 리사이즈 (원본 비율 유지 X)\n",
    "    img_resized = cv2.resize(img, (YOLO_SIZE, YOLO_SIZE), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "    # YOLO 모델 입력 형식 변환\n",
    "    img_yolo = cv2.cvtColor(img_resized, cv2.COLOR_BGR2RGB).astype(np.float32) / 255.0\n",
    "    img_yolo = np.expand_dims(img_yolo, axis=0)  \n",
    "\n",
    "    return img_yolo, img_resized  # YOLO 입력용 이미지와 원본 리사이즈된 이미지 반환\n",
    "\n",
    "# 세그멘테이션 실행 함수\n",
    "def run_inference(image_path):\n",
    "    input_shape = input_details[0]['shape']\n",
    "    \n",
    "    # 이미지 전처리 (YOLO 입력 크기 유지)\n",
    "    input_data, resized_image = preprocess_image(image_path, input_shape)\n",
    "    \n",
    "    # 모델에 입력 데이터 설정\n",
    "    interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "    \n",
    "    # 모델 실행\n",
    "    interpreter.invoke()\n",
    "    \n",
    "    # 결과 가져오기\n",
    "    segmentation_output = interpreter.get_tensor(output_details[0]['index'])\n",
    "\n",
    "    # 첫 번째 클래스 선택\n",
    "    segmentation_mask = segmentation_output[0, 0, :, :]  \n",
    "\n",
    "    # Threshold 적용\n",
    "    segmentation_mask = (segmentation_mask > THRESHOLD).astype(np.uint8) * 255  \n",
    "\n",
    "    # Gaussian Blur 적용\n",
    "    segmentation_mask = cv2.GaussianBlur(segmentation_mask, (5, 5), 0)\n",
    "\n",
    "    # 원본 크기에 맞춰 리사이즈 (원본 비율이 아닌 YOLO 해상도로 유지)\n",
    "    mask_overlay = cv2.resize(segmentation_mask, (YOLO_SIZE, YOLO_SIZE), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "    # 컬러 마스크 생성\n",
    "    mask_colored = np.zeros((YOLO_SIZE, YOLO_SIZE, 3), dtype=np.uint8)\n",
    "    mask_colored[mask_overlay > 128] = [0, 255, 0]  \n",
    "\n",
    "    # 원본 이미지와 투명한 마스크 합성\n",
    "    overlay_image = cv2.addWeighted(resized_image, 1 - ALPHA, mask_colored, ALPHA, 0)\n",
    "\n",
    "    return overlay_image\n",
    "\n",
    "# 실행 테스트\n",
    "image_path = \"test.jpg\"\n",
    "overlay_result = run_inference(image_path)\n",
    "\n",
    "# 결과 시각화\n",
    "cv2.imshow(\"Segmentation Overlay\", overlay_result)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03. Tracking\n",
    "## 03-1. Tracking 개념 및 적용\n",
    "### Object Tracking이란?\n",
    "Tracking(추적)은 비디오 프레임에서 객체를 식별하고 연속된 프레임에서 해당 객체의 위치를 추적하는 기술입니다. 컴퓨터 비전에서는 다양한 방식의 트래킹 기법이 존재하며, 각각의 알고리즘은 상황에 따라 적절하게 사용됩니다.\n",
    "- **Single Object Tracking(SOT)**: 한 개의 객체를 추적 (e.g., KCF, CSRT)\n",
    "- **Multi-Object Tracking(MOT)**: 여러 객체를 동시에 추적 (e.g., SORT, DeepSORT, ByteTrack)\n",
    "\n",
    "## 03-2. Tracking의 기본 원리\n",
    "트래킹은 기본적으로 객체 탐지(Object Detection) + 연속된 프레임에서의 위치 예측으로 이루어집니다.\n",
    "\n",
    "### 객체 탐지 (Object Detection)\n",
    "객체를 추적하려면 먼저 객체를 탐지해야 합니다. 일반적으로 YOLO, Faster R-CNN, SSD 같은 딥러닝 기반 모델이 사용됩니다.\n",
    "- **한 번만 탐지(One-shot Detection)**: 첫 프레임에서만 객체를 탐지하고 이후에는 위치를 예측하여 추적 (e.g., Kalman Filter)\n",
    "- **매 프레임 탐지(Frame-by-Frame Detection)**: 매 프레임마다 객체 탐지를 수행하여 위치를 갱신 (e.g., YOLO + Tracking)\n",
    "\n",
    "### 객체 매칭 (Object Association)\n",
    "이전 프레임과 현재 프레임의 객체를 연결하는 과정입니다. 트래킹에서는 다음과 같은 방식이 사용됩니다.\n",
    "- **IOU (Intersection over Union)**: 두 개의 바운딩 박스의 겹치는 영역을 비교하여 같은 객체로 인식\n",
    "- **Feature Matching**: 객체의 특징점(ORB, SIFT, SURF)을 비교하여 동일한 객체 판별\n",
    "- **딥러닝 기반 Re-identification (Re-ID)**: CNN을 이용하여 객체를 구별하고 동일 객체인지 판별\n",
    "\n",
    "### 예측 및 보정\n",
    "객체의 위치는 다음 프레임에서 어디에 있을지를 예측하고, 이후 실제 측정값을 기반으로 보정됩니다.\n",
    "- **칼만 필터 (Kalman Filter)**: 선형적으로 객체의 움직임을 예측하고 보정\n",
    "- **파티클 필터 (Particle Filter)**: 확률 분포를 활용하여 비선형적인 움직임까지 추적 가능\n",
    "- **LSTM 기반 예측**: 객체의 이동 패턴을 학습하여 예측 정확도를 높임\n",
    "\n",
    "### 객체 추적의 필요성\n",
    "객체 추적은 다음과 같은 다양한 분야에서 활용됩니다.\n",
    "- **자율 주행 차량**: 차량과 보행자의 움직임을 추적하여 안전한 주행을 지원\n",
    "- **스포츠 분석**: 선수들의 움직임을 추적하여 경기 데이터를 분석\n",
    "- **감시 시스템**: 특정 인물 또는 차량의 이동을 모니터링\n",
    "- **로봇 비전**: 물체를 따라가거나 특정한 대상과 상호작용\n",
    "- **증강 현실 (AR)**: 실시간으로 사용자의 움직임을 추적하여 자연스러운 인터페이스 제공\n",
    "\n",
    "## 03-3. Tracking 알고리즘\n",
    "객체 추적을 위한 대표적인 알고리즘에는 여러 가지가 있으며, 용도에 따라 적절한 방식을 선택해야 합니다.\n",
    "\n",
    "### 전통적인 트래킹 방법\n",
    "| 알고리즘 | 설명 |\n",
    "|----------|------|\n",
    "| **Mean-Shift** | 히스토그램을 기반으로 이동하는 객체의 위치를 찾음 |\n",
    "| **CamShift** | Mean-Shift를 개선하여 크기 변화까지 추적 가능 |\n",
    "| **Optical Flow** | 프레임 간 픽셀 이동을 기반으로 객체의 움직임을 추적 |\n",
    "| **Kalman Filter** | 선형 움직임을 가정하고 객체의 다음 위치를 예측 |\n",
    "| **Particle Filter** | 확률 모델을 활용하여 비선형 움직임을 추적 |\n",
    "\n",
    "### 딥러닝 기반 트래킹 방법\n",
    "| 알고리즘 | 설명 |\n",
    "|----------|------|\n",
    "| **SORT (Simple Online and Realtime Tracker)** | Kalman Filter + IOU 기반 추적 |\n",
    "| **DeepSORT** | SORT에 Re-ID를 추가하여 재인식 성능 향상 |\n",
    "| **ByteTrack** | SORT의 탐지를 개선하여 신뢰도 낮은 객체도 추적 |\n",
    "| **FairMOT** | Multi-Object Tracking을 위한 단일 네트워크 적용 |\n",
    "| **Siamese Networks** | 쌍둥이 네트워크를 사용하여 객체를 계속 인식 |\n",
    "| **Transformer 기반 트래킹 (TrackFormer)** | Transformer 모델을 활용한 최신 트래킹 방식 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 04. [실습 9] 라즈베리파이에서 Multi-Object Tracking 적용\n",
    "1. YOLO + SORT를 적용한 MOT 구현\n",
    "2. OpenCV 기반 실시간 Tracking 수행\n",
    "3. 성능 비교 및 최적화 방안 분석"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 필수 라이브러리 설치\n",
    "```bash\n",
    "sudo apt update && sudo apt upgrade -y\n",
    "sudo apt install python3-pip python3-opencv libatlas-base-dev -y\n",
    "pip3 install torch torchvision numpy opencv-python\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### YOLOv5 설치\n",
    "```bash\n",
    "git clone https://github.com/ultralytics/yolov5.git\n",
    "cd yolov5\n",
    "pip install -r requirements.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5n.pt to yolov5n.pt...\n",
      "100%|██████████| 3.87M/3.87M [00:01<00:00, 3.66MB/s]\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5n summary: 213 layers, 1867405 parameters, 0 gradients, 4.5 GFLOPs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOLOv5 모델 로드 완료!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# yolov5 폴더를 경로에 추가\n",
    "sys.path.append(\"yolov5\")\n",
    "\n",
    "from yolov5.models.common import DetectMultiBackend\n",
    "from yolov5.utils.general import non_max_suppression\n",
    "\n",
    "# 라즈베리파이 CPU 모드\n",
    "device = torch.device(\"cpu\")  # 문자열 \"cpu\"가 아니라 torch.device 객체 사용\n",
    "model = DetectMultiBackend(\"yolov5n.pt\", device=device)\n",
    "model.eval()\n",
    "\n",
    "print(\"YOLOv5 모델 로드 완료!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tracking 라이브러리 설치\n",
    "\n",
    "```bash\n",
    "pip install supervision\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5 🚀 v7.0-398-g5cdad892 Python-3.10.12 torch-2.6.0+cu124 CPU\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5n summary: 213 layers, 1867405 parameters, 0 gradients, 4.5 GFLOPs\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "import supervision as sv\n",
    "\n",
    "sys.path.append(\"yolov5\")\n",
    "from yolov5.models.common import DetectMultiBackend\n",
    "from yolov5.utils.general import non_max_suppression\n",
    "from yolov5.utils.torch_utils import select_device\n",
    "\n",
    "# YOLOv5 모델 로드\n",
    "device = select_device('cpu')  # 라즈베리파이에서는 CPU 사용\n",
    "model = DetectMultiBackend(\"yolov5n.pt\", device=device)\n",
    "model.eval()\n",
    "\n",
    "# ByteTrack 트래커 초기화\n",
    "tracker = sv.ByteTrack()\n",
    "\n",
    "# ✅ 객체의 궤적(trajectory)을 저장할 딕셔너리\n",
    "track_history = {}\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # YOLOv5 입력 전처리\n",
    "    img = cv2.resize(frame, (640, 640))\n",
    "    img = img[:, :, ::-1].transpose(2, 0, 1)  # BGR → RGB 변환\n",
    "    img = np.ascontiguousarray(img, dtype=np.float32) / 255.0  # 정규화\n",
    "    img = torch.from_numpy(img).unsqueeze(0).to(device)\n",
    "\n",
    "    # 객체 탐지 수행\n",
    "    with torch.no_grad():\n",
    "        preds = model(img)\n",
    "        preds = non_max_suppression(preds, conf_thres=0.4, iou_thres=0.5)[0]\n",
    "\n",
    "    # 트래킹을 위한 Detections 객체 생성\n",
    "    if preds is not None and len(preds) > 0:\n",
    "        xyxy = []\n",
    "        confidences = []\n",
    "        class_ids = []\n",
    "\n",
    "        for det in preds:\n",
    "            x1, y1, x2, y2, conf, cls = det[:6]\n",
    "            xyxy.append([x1, y1, x2, y2])\n",
    "            confidences.append(conf.item())  # Tensor → float 변환\n",
    "            class_ids.append(int(cls))\n",
    "\n",
    "        detections = sv.Detections(\n",
    "            xyxy=np.array(xyxy),\n",
    "            confidence=np.array(confidences),\n",
    "            class_id=np.array(class_ids),\n",
    "        )\n",
    "\n",
    "        # 트래킹 업데이트\n",
    "        tracks = tracker.update_with_detections(detections)\n",
    "\n",
    "        # 트래킹된 객체에 ID와 bounding box 표시\n",
    "        for i in range(len(tracks.xyxy)):\n",
    "            x1, y1, x2, y2 = map(int, tracks.xyxy[i])  # 바운딩 박스 좌표\n",
    "            track_id = tracks.tracker_id[i]  # 트래킹 ID\n",
    "            \n",
    "            # ✅ ID별로 궤적(trajectory) 저장\n",
    "            center_x = (x1 + x2) // 2\n",
    "            center_y = (y1 + y2) // 2\n",
    "\n",
    "            if track_id not in track_history:\n",
    "                track_history[track_id] = []\n",
    "\n",
    "            track_history[track_id].append((center_x, center_y))\n",
    "\n",
    "            # ✅ 궤적(trajectory) 선 그리기\n",
    "            for j in range(1, len(track_history[track_id])):\n",
    "                if track_history[track_id][j - 1] is None or track_history[track_id][j] is None:\n",
    "                    continue\n",
    "                cv2.line(frame, track_history[track_id][j - 1], track_history[track_id][j], (0, 255, 255), 2)\n",
    "\n",
    "            # 바운딩 박스 그리기\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "            cv2.putText(frame, f\"ID: {track_id}\", (x1, y1 - 10),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "    # 화면 출력\n",
    "    cv2.imshow(\"YOLOv5 + Supervision Tracking + Trajectory\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
