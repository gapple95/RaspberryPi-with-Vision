{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01. YOLOë¥¼ í™œìš©í•œ ì‹¤ì‹œê°„ ê°ì²´ íƒì§€ (Detection & Classification)\n",
    "\n",
    "## 01-1. YOLO ê°œë… ë° ë‹¤ì–‘í•œ í™œìš© ë°©ì‹\n",
    "\n",
    "### YOLOë€?\n",
    "- YOLO(You Only Look Once)ëŠ” ë‹¨ì¼ ì‹ ê²½ë§ì„ ì‚¬ìš©í•˜ì—¬ í•œ ë²ˆì˜ ì—°ì‚°ìœ¼ë¡œ ê°ì²´ì˜ ìœ„ì¹˜ì™€ í´ë˜ìŠ¤ ì •ë³´ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ê°ì²´ íƒì§€ ëª¨ë¸ì…ë‹ˆë‹¤.\n",
    "- ê¸°ì¡´ì˜ R-CNN ë° Faster R-CNNê³¼ ê°™ì€ ëª¨ë¸ì€ í›„ë³´ ì˜ì—­ì„ ë¨¼ì € ì°¾ê³  ì´ë¥¼ ë¶„ë¥˜í•˜ëŠ” ë°©ì‹ì´ì§€ë§Œ, YOLOëŠ” ì „ì²´ ì´ë¯¸ì§€ë¥¼ í•œ ë²ˆì— ë¶„ì„í•˜ì—¬ ì˜ˆì¸¡ì„ ìˆ˜í–‰í•˜ë¯€ë¡œ ì†ë„ê°€ ë¹ ë¦…ë‹ˆë‹¤.\n",
    "\n",
    "### YOLOì˜ ì‘ë™ ë°©ì‹\n",
    "- YOLOëŠ” CNN(Convolutional Neural Network) ê¸°ë°˜ìœ¼ë¡œ ë™ì‘í•˜ë©°, ì…ë ¥ ì´ë¯¸ì§€ë¥¼ íŠ¹ì§• ë§µìœ¼ë¡œ ë³€í™˜í•œ í›„ ì—¬ëŸ¬ ê°œì˜ ê²©ìë¡œ ë‚˜ëˆ„ì–´ ê° ê²©ìê°€ ê°ì²´ì˜ ë°”ìš´ë”© ë°•ìŠ¤ì™€ í´ë˜ìŠ¤ í™•ë¥ ì„ ì˜ˆì¸¡í•©ë‹ˆë‹¤.\n",
    "- ê¸°ì¡´ R-CNN ê¸°ë°˜ íƒì§€ ëª¨ë¸ë³´ë‹¤ ì†ë„ê°€ ë¹ ë¥´ê³ , ì‹¤ì‹œê°„ ì²˜ë¦¬ì— ì í•©í•©ë‹ˆë‹¤.\n",
    "\n",
    "### YOLO ë²„ì „ë³„ íŠ¹ì§• ë° ì°¨ì´ì \n",
    "- **YOLOv1~v3**: ì´ˆê¸° YOLO ëª¨ë¸ë¡œ, ì†ë„ê°€ ë¹ ë¥´ì§€ë§Œ ì •í™•ë„ê°€ ìƒëŒ€ì ìœ¼ë¡œ ë‚®ì•˜ìŠµë‹ˆë‹¤.\n",
    "- **YOLOv4**: CSPDarknet53 ë°±ë³¸ì„ ë„ì…í•˜ì—¬ ì •í™•ë„ë¥¼ í–¥ìƒì‹œí‚¨ ë²„ì „ì…ë‹ˆë‹¤.\n",
    "- **YOLOv5**: PyTorch ê¸°ë°˜ìœ¼ë¡œ êµ¬í˜„ë˜ì—ˆìœ¼ë©°, ê²½ëŸ‰í™”ëœ ëª¨ë¸(Tiny, Nano ë“±)ì„ í¬í•¨í•˜ì—¬ ë‹¤ì–‘í•œ í™˜ê²½ì—ì„œ í™œìš©í•  ìˆ˜ ìˆë„ë¡ ìµœì í™”ë˜ì—ˆìŠµë‹ˆë‹¤.\n",
    "- **YOLOv6~v8**: ìµœì‹  YOLO ë²„ì „ìœ¼ë¡œ, ì†ë„ì™€ ì •í™•ë„ë¥¼ ê· í˜• ìˆê²Œ ê°œì„ í•˜ë©°, Segmentation ë° Classification ê¸°ëŠ¥ë„ ì§€ì›í•©ë‹ˆë‹¤.\n",
    "\n",
    "### Tiny ëª¨ë¸ê³¼ Nano ëª¨ë¸ì˜ ì°¨ì´ì  ë° ì„ íƒ ì´ìœ \n",
    "- **YOLO Tiny**: YOLOì˜ ê²½ëŸ‰í™”ëœ ë²„ì „ìœ¼ë¡œ, ì—°ì‚°ëŸ‰ì„ ì¤„ì—¬ ë¹ ë¥¸ ì†ë„ë¥¼ ì œê³µí•˜ì§€ë§Œ ì •í™•ë„ëŠ” ë‚®ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "- **YOLO Nano**: YOLOv5 ë° v8ì—ì„œ ë“±ì¥í•œ ì´ˆê²½ëŸ‰ ëª¨ë¸ë¡œ, ê²½ëŸ‰í™”ì™€ ì„±ëŠ¥ì˜ ê· í˜•ì„ ë§ì¶”ë©°, Raspberry Piì™€ ê°™ì€ ì—£ì§€ ë””ë°”ì´ìŠ¤ì—ì„œë„ ì›í™œí•˜ê²Œ ë™ì‘í•  ìˆ˜ ìˆë„ë¡ ìµœì í™”ë˜ì—ˆìŠµë‹ˆë‹¤.\n",
    "- **Raspberry Pi ê°•ì˜ì—ì„œ ì‚¬ìš©í•  ëª¨ë¸**: Raspberry Piì˜ ì—°ì‚° ëŠ¥ë ¥ì„ ê³ ë ¤í•˜ì—¬ YOLOv5 Nano ëª¨ë¸ì„ ì„ íƒí•˜ë©°, ì´ë¥¼ TFLiteë¡œ ë³€í™˜í•˜ì—¬ ê²½ëŸ‰í™”ëœ í˜•íƒœë¡œ ì‹¤í–‰í•  ê²ƒì…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02. [ì‹¤ìŠµ 6] YOLOv5-Nano ëª¨ë¸ ë³€í™˜ ë° ì‹¤í–‰\n",
    "\n",
    "## 02-1. Raspberry Piì—ì„œ ì‹¤í–‰ ì‹œ í•„ìš”í•œ ì„¤ì • ë° í™˜ê²½ êµ¬ì¶•\n",
    "```bash\n",
    "git clone https://github.com/ultralytics/yolov5.git\n",
    "cd yolov5\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "## 02-2. YOLOv5 Nano ëª¨ë¸ì„ TFLiteë¡œ ë³€í™˜\n",
    "### YOLOv5n ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ë° ë³€í™˜\n",
    "\n",
    "```bash\n",
    "python3 export.py --weights yolov5n.pt --include tflite\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ëª¨ë¸ í™•ì¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì…ë ¥ ì •ë³´: [{'name': 'serving_default_keras_tensor_121:0', 'index': 0, 'shape': array([  1, 640, 640,   3], dtype=int32), 'shape_signature': array([  1, 640, 640,   3], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\n",
      "ì¶œë ¥ ì •ë³´: [{'name': 'StatefulPartitionedCall_1:0', 'index': 530, 'shape': array([    1, 25200,    85], dtype=int32), 'shape_signature': array([    1, 25200,    85], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\n",
      "ì¶œë ¥ ë°ì´í„°: (1, 25200, 85)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# TFLite ëª¨ë¸ ë¡œë“œ\n",
    "interpreter = tf.lite.Interpreter(model_path=\"yolov5/yolov5n-fp16.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# ì…ë ¥/ì¶œë ¥ ì •ë³´ í™•ì¸\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "print(\"ì…ë ¥ ì •ë³´:\", input_details)\n",
    "print(\"ì¶œë ¥ ì •ë³´:\", output_details)\n",
    "\n",
    "# ëœë¤ ì…ë ¥ ë°ì´í„° ìƒì„± (batch=1, height=640, width=640, channel=3) - ì˜¬ë°”ë¥¸ í˜•ì‹\n",
    "input_data = np.random.rand(1, 640, 640, 3).astype(np.float32)  # (1, 640, 640, 3)\n",
    "\n",
    "# ì…ë ¥ ë°ì´í„° ì„¤ì • ë° ì‹¤í–‰\n",
    "interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "interpreter.invoke()\n",
    "\n",
    "# ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°\n",
    "output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "print(\"ì¶œë ¥ ë°ì´í„°:\", output_data.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02-3. ì‹¤ì‹œê°„ ì¹´ë©”ë¼ ìŠ¤íŠ¸ë¦¬ë°ê³¼ ì—°ê²°í•˜ì—¬ ê°ì²´ íƒì§€"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ë¼ì´ë¸ŒëŸ¬ë¦¬ ë° í™˜ê²½ ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# TFLite ëª¨ë¸ ë¡œë“œ\n",
    "interpreter = tf.lite.Interpreter(model_path=\"yolov5/yolov5n-fp16.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# ì…ë ¥ / ì¶œë ¥ í…ì„œ ì •ë³´ ê°€ì ¸ì˜¤ê¸°\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# YOLOv5 Nano ëª¨ë¸ ì •ë³´\n",
    "INPUT_WIDTH = 640\n",
    "INPUT_HEIGHT = 640\n",
    "\n",
    "# í´ë˜ìŠ¤ ì´ë¦„ (YOLOv5 ê¸°ë³¸ COCO ë°ì´í„°ì…‹ ê¸°ì¤€)\n",
    "COCO_CLASSES = [\n",
    "    \"person\", \"bicycle\", \"car\", \"motorcycle\", \"airplane\", \"bus\", \"train\", \"truck\",\n",
    "    \"boat\", \"traffic light\", \"fire hydrant\", \"stop sign\", \"parking meter\", \"bench\",\n",
    "    \"bird\", \"cat\", \"dog\", \"horse\", \"sheep\", \"cow\", \"elephant\", \"bear\", \"zebra\",\n",
    "    \"giraffe\", \"backpack\", \"umbrella\", \"handbag\", \"tie\", \"suitcase\", \"frisbee\",\n",
    "    \"skis\", \"snowboard\", \"sports ball\", \"kite\", \"baseball bat\", \"baseball glove\",\n",
    "    \"skateboard\", \"surfboard\", \"tennis racket\", \"bottle\", \"wine glass\", \"cup\",\n",
    "    \"fork\", \"knife\", \"spoon\", \"bowl\", \"banana\", \"apple\", \"sandwich\", \"orange\",\n",
    "    \"broccoli\", \"carrot\", \"hot dog\", \"pizza\", \"donut\", \"cake\", \"chair\", \"couch\",\n",
    "    \"potted plant\", \"bed\", \"dining table\", \"toilet\", \"TV\", \"laptop\", \"mouse\",\n",
    "    \"remote\", \"keyboard\", \"cell phone\", \"microwave\", \"oven\", \"toaster\", \"sink\",\n",
    "    \"refrigerator\", \"book\", \"clock\", \"vase\", \"scissors\", \"teddy bear\", \"hair drier\",\n",
    "    \"toothbrush\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ê°ì²´ íƒì§€ í•¨ìˆ˜ ì •ì˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê°ì²´ íƒì§€ í•¨ìˆ˜ (TFLite)\n",
    "def detect_objects(image, conf_threshold=0.9, iou_threshold=0.6):\n",
    "    # ì›ë³¸ ì´ë¯¸ì§€ í¬ê¸° ì €ì¥\n",
    "    orig_height, orig_width, _ = image.shape\n",
    "\n",
    "    # ì´ë¯¸ì§€ ì „ì²˜ë¦¬ (TFLite í˜•ì‹ì— ë§ê²Œ ë³€í™˜)\n",
    "    input_image = cv2.resize(image, (INPUT_WIDTH, INPUT_HEIGHT))\n",
    "    input_image = input_image.astype(np.float32) / 255.0  # ì •ê·œí™”\n",
    "    input_image = np.expand_dims(input_image, axis=0)  # ë°°ì¹˜ ì°¨ì› ì¶”ê°€\n",
    "\n",
    "    # ëª¨ë¸ ì…ë ¥ ì„¤ì •\n",
    "    interpreter.set_tensor(input_details[0]['index'], input_image)\n",
    "\n",
    "    # ì¶”ë¡  ì‹¤í–‰\n",
    "    interpreter.invoke()\n",
    "\n",
    "    # YOLOv5 TFLite ë‹¨ì¼ ì¶œë ¥ ì²˜ë¦¬\n",
    "    output_data = interpreter.get_tensor(output_details[0]['index'])  # (1, 25200, 85)\n",
    "    output_data = np.squeeze(output_data)  # (25200, 85)ë¡œ ë³€í™˜\n",
    "\n",
    "    # ë°”ìš´ë”© ë°•ìŠ¤ ë° ì‹ ë¢°ë„ ì¶”ì¶œ\n",
    "    boxes = output_data[:, :4]  # x_center, y_center, w, h\n",
    "    confidences = output_data[:, 4]  # confidence score\n",
    "    class_probs = output_data[:, 5:]  # í´ë˜ìŠ¤ í™•ë¥ ê°’ (80ê°œ)\n",
    "\n",
    "    # í´ë˜ìŠ¤ ë° ì ìˆ˜ ì¶”ì¶œ\n",
    "    class_ids = np.argmax(class_probs, axis=1)  # ê°€ì¥ ë†’ì€ í™•ë¥ ì˜ í´ë˜ìŠ¤ ì„ íƒ\n",
    "    scores = np.max(class_probs, axis=1)  # í•´ë‹¹ í´ë˜ìŠ¤ì˜ ì‹ ë¢°ë„ ì ìˆ˜\n",
    "\n",
    "    # ì‹ ë¢°ë„ ê¸°ì¤€ ì ìš©\n",
    "    indices = np.where(scores > conf_threshold)[0]  # ì‹ ë¢°ë„ 0.5 ì´ìƒ ê°ì²´ë§Œ ì„ íƒ\n",
    "    boxes, scores, class_ids = boxes[indices], scores[indices], class_ids[indices]\n",
    "\n",
    "    # ë°”ìš´ë”© ë°•ìŠ¤ ì¢Œí‘œ ë³€í™˜ (x_center, y_center, w, h â†’ x1, y1, x2, y2)\n",
    "    scaled_boxes = []\n",
    "    for i in range(len(boxes)):\n",
    "        x_center, y_center, w, h = boxes[i]\n",
    "        x1 = int((x_center - w / 2) * orig_width)\n",
    "        y1 = int((y_center - h / 2) * orig_height)\n",
    "        x2 = int((x_center + w / 2) * orig_width)\n",
    "        y2 = int((y_center + h / 2) * orig_height)\n",
    "        scaled_boxes.append([x1, y1, x2, y2])\n",
    "\n",
    "    # Non-Maximum Suppression (NMS) ì ìš©í•˜ì—¬ ì¤‘ë³µ ë°•ìŠ¤ ì œê±°\n",
    "    indices = cv2.dnn.NMSBoxes(scaled_boxes, scores.tolist(), conf_threshold, iou_threshold)\n",
    "    \n",
    "    detected_objects = []\n",
    "    if len(indices) > 0:\n",
    "        for i in indices.flatten():\n",
    "            class_name = COCO_CLASSES[class_ids[i]] if class_ids[i] < len(COCO_CLASSES) else \"Unknown\"\n",
    "            detected_objects.append((*scaled_boxes[i], scores[i], class_name))\n",
    "\n",
    "    return detected_objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ì›¹ìº ì—ì„œ ê°ì²´ íƒì§€ ì‹¤í–‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # ê°ì²´ íƒì§€ ì‹¤í–‰\n",
    "    objects = detect_objects(frame)\n",
    "\n",
    "    # íƒì§€ëœ ê°ì²´ë¥¼ í™”ë©´ì— í‘œì‹œ\n",
    "    for (x1, y1, x2, y2, confidence, class_name) in objects:\n",
    "        label = f\"{class_name}: {confidence:.2f}\"\n",
    "        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "        cv2.putText(frame, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "    # í™”ë©´ì— ì¶œë ¥\n",
    "    cv2.imshow(\"YOLOv5n TFLite Object Detection\", frame)\n",
    "\n",
    "    # q í‚¤ë¥¼ ëˆ„ë¥´ë©´ ì¢…ë£Œ\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03. YOLOë¥¼ í™œìš©í•œ ì´ë¯¸ì§€ ë¶„ë¥˜(Classification)\n",
    "\n",
    "## 03-1. YOLOë¥¼ ì´ë¯¸ì§€ ë¶„ë¥˜ìš©ìœ¼ë¡œ í™œìš©í•˜ëŠ” ë°©ë²•\n",
    "- YOLOëŠ” ê¸°ë³¸ì ìœ¼ë¡œ ê°ì²´ íƒì§€ ëª¨ë¸ì´ì§€ë§Œ, ë¶„ë¥˜(Classification) ì „ìš© êµ¬ì¡°ë¡œ í™œìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "- YOLOì˜ ë¶„ë¥˜ ëª¨ë¸ì€ ê¸°ì¡´ CNN ê¸°ë°˜ ë¶„ë¥˜ ëª¨ë¸(ResNet, VGG ë“±)ê³¼ ë‹¬ë¦¬, ê²½ëŸ‰í™”ë˜ì–´ ë¹ ë¥¸ ì†ë„ë¥¼ ì œê³µí•©ë‹ˆë‹¤.\n",
    "- YOLOëŠ” Backbone(íŠ¹ì§• ì¶”ì¶œ ë„¤íŠ¸ì›Œí¬)ì„ í™œìš©í•˜ì—¬ ë¶„ë¥˜ íƒœìŠ¤í¬ë¥¼ ìˆ˜í–‰í•˜ë©°, ê³ í•´ìƒë„ ì…ë ¥ ì´ë¯¸ì§€ì— ëŒ€í•´ ë¹ ë¥´ê³  ì •í™•í•œ ì˜ˆì¸¡ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.\n",
    "- YOLOv5 ë° YOLOv8ì—ì„œëŠ” Classification ì „ìš© í•™ìŠµ ê¸°ëŠ¥ì´ ì œê³µë˜ë©°, ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ì„ í†µí•´ íŠ¹ì • ë°ì´í„°ì…‹ì— ë§ì¶° ë¯¸ì„¸ ì¡°ì •(Fine-Tuning)í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.\n",
    "- YOLOì˜ ë¶„ë¥˜ ëª¨ë¸ì€ ë‹¨ìˆœí•œ ì´ë¯¸ì§€ ë¶„ë¥˜ë¿ë§Œ ì•„ë‹ˆë¼, ì‚°ì—…ìš© ê²€ì‚¬, ì˜ë£Œ ì˜ìƒ ë¶„ì„, í’ˆì§ˆ ê´€ë¦¬ ë“± ë‹¤ì–‘í•œ ì‹¤ì „ ì‘ìš© ë¶„ì•¼ì—ì„œ í™œìš©ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "- YOLOëŠ” ê²½ëŸ‰í™”ëœ êµ¬ì¡° ë•ë¶„ì— ì—£ì§€ ë””ë°”ì´ìŠ¤(Raspberry Pi, Jetson Nano ë“±)ì—ì„œë„ ì›í™œí•˜ê²Œ ë™ì‘í•  ìˆ˜ ìˆìœ¼ë©°, ì‹¤ì‹œê°„ ì´ë¯¸ì§€ ë¶„ë¥˜ë¥¼ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤.\n",
    "\n",
    "## 03-2. YOLO ê¸°ë°˜ ë¶„ë¥˜ ëª¨ë¸ì˜ í™œìš© ì‚¬ë¡€\n",
    "- **ì˜ë£Œ ì˜ìƒ ë¶„ì„**: X-ray ë˜ëŠ” MRI ì´ë¯¸ì§€ë¥¼ ë¶„ì„í•˜ì—¬ ì§ˆë³‘ì„ ë¶„ë¥˜í•˜ëŠ” ë° í™œìš©\n",
    "- **ì œì¡°ì—… í’ˆì§ˆ ê´€ë¦¬**: ì œí’ˆì˜ ë¶ˆëŸ‰ ì—¬ë¶€ë¥¼ ì‹¤ì‹œê°„ìœ¼ë¡œ ê°ì§€í•˜ì—¬ ê³µì • ìë™í™” ê°€ëŠ¥\n",
    "- **ì†Œë§¤ ë° ë³´ì•ˆ ì‹œìŠ¤í…œ**: ì–¼êµ´ ì¸ì‹ì„ í†µí•œ ì‹ ì› í™•ì¸ ë° ê³ ê° í–‰ë™ ë¶„ì„\n",
    "- **ì¼ë°˜ì ì¸ ì´ë¯¸ì§€ ë¶„ë¥˜**: ê°œ, ê³ ì–‘ì´ ë¶„ë¥˜ì™€ ê°™ì€ ì¼ë°˜ì ì¸ ì´ë¯¸ì§€ ë¶„ë¥˜ íƒœìŠ¤í¬ ì ìš©"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 04. [ì‹¤ìŠµ 7] YOLOë¥¼ í™œìš©í•œ ì´ë¯¸ì§€ ë¶„ë¥˜\n",
    "\n",
    "## 04-1. ì‚¬ì „ í•™ìŠµëœ YOLO ë¶„ë¥˜ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ë° ì‹¤í–‰\n",
    "\n",
    "### Ultralyticsì„ í™œìš©í•œ tflite ëª¨ë¸ ë‹¤ìš´ë¡œë“œ\n",
    "ì´ì „ ì‹¤ìŠµ ë°©ì‹ì—ì„  ì§ì ‘ gitì„ ë¶ˆëŸ¬ì™€ convertë¥¼ ì§„í–‰í–ˆì§€ë§Œ, YOLOv8 ë²„ì „ë¶€í„°ëŠ” Ultralytics ë¼ì´ë¸ŒëŸ¬ë¦¬í™œìš©í•˜ì—¬ ê°„ë‹¨íˆ ë³€í™˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.71 ğŸš€ Python-3.10.12 torch-2.6.0+cu124 CPU (12th Gen Intel Core(TM) i5-12400F)\n",
      "YOLOv8n-cls summary (fused): 73 layers, 2,715,880 parameters, 0 gradients, 4.3 GFLOPs\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'yolov8n-cls.pt' with input shape (1, 3, 224, 224) BCHW and output shape(s) (1, 1000) (5.3 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mTensorFlow SavedModel:\u001b[0m starting export with tensorflow 2.18.0...\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.17.0 opset 19...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m slimming with onnxslim 0.1.48...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m export success âœ… 0.2s, saved as 'yolov8n-cls.onnx' (10.4 MB)\n",
      "\u001b[34m\u001b[1mTensorFlow SavedModel:\u001b[0m starting TFLite export with onnx2tf 1.26.3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1739403119.766232    5193 devices.cc:67] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\n",
      "I0000 00:00:1739403119.766300    5193 single_machine.cc:361] Starting new session\n",
      "W0000 00:00:1739403119.917994    5193 tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format.\n",
      "W0000 00:00:1739403119.918011    5193 tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency.\n",
      "2025-02-13 08:32:00.035890: I tensorflow/compiler/mlir/lite/flatbuffer_export.cc:3893] Estimated count of arithmetic ops: 477.397 M  ops, equivalently 238.698 M  MACs\n",
      "I0000 00:00:1739403120.148118    5193 devices.cc:67] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\n",
      "I0000 00:00:1739403120.148187    5193 single_machine.cc:361] Starting new session\n",
      "W0000 00:00:1739403120.297677    5193 tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format.\n",
      "W0000 00:00:1739403120.297692    5193 tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency.\n",
      "2025-02-13 08:32:00.428296: I tensorflow/compiler/mlir/lite/flatbuffer_export.cc:3893] Estimated count of arithmetic ops: 477.397 M  ops, equivalently 238.698 M  MACs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mTensorFlow SavedModel:\u001b[0m export success âœ… 8.8s, saved as 'yolov8n-cls_saved_model' (26.2 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mTensorFlow Lite:\u001b[0m starting export with tensorflow 2.18.0...\n",
      "\u001b[34m\u001b[1mTensorFlow Lite:\u001b[0m export success âœ… 0.0s, saved as 'yolov8n-cls_saved_model/yolov8n-cls_float32.tflite' (10.4 MB)\n",
      "\n",
      "Export complete (9.0s)\n",
      "Results saved to \u001b[1m/home/paymentinapp/raspberrypi/04_YOLOë¥¼_í™œìš©í•œ_ì‹¤ì‹œê°„_ê°ì²´_íƒì§€\u001b[0m\n",
      "Predict:         yolo predict task=classify model=yolov8n-cls_saved_model/yolov8n-cls_float32.tflite imgsz=224  \n",
      "Validate:        yolo val task=classify model=yolov8n-cls_saved_model/yolov8n-cls_float32.tflite imgsz=224 data=../datasets/imagenet  \n",
      "Visualize:       https://netron.app\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'yolov8n-cls_saved_model/yolov8n-cls_float32.tflite'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import tensorflow as tf\n",
    "from ultralytics import YOLO\n",
    "\n",
    "model = YOLO('yolov8n-cls.pt')  # ì‚¬ì „ í•™ìŠµëœ YOLOv8 ë¶„ë¥˜ ëª¨ë¸ ë¡œë“œ\n",
    "model.export(format='tflite')  # tfliteë¡œ ë³€í™˜"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 04-2. ë¼ì¦ˆë² ë¦¬íŒŒì´ì—ì„œ ì‹¤ì‹œê°„ ì¹´ë©”ë¼ë¥¼ í™œìš©í•œ ì´ë¯¸ì§€ ë¶„ë¥˜\n",
    "\n",
    "### ì‹¤ì‹œê°„ìœ¼ë¡œ YOLO ë¶„ë¥˜ ëª¨ë¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# TFLite ëª¨ë¸ ë¡œë“œ\n",
    "interpreter = tf.lite.Interpreter(model_path=\"./yolov8n-cls_saved_model/yolov8n-cls_float32.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# ì…ë ¥ ë° ì¶œë ¥ í…ì„œ ì •ë³´ ê°€ì ¸ì˜¤ê¸°\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# ëª¨ë¸ì´ ê¸°ëŒ€í•˜ëŠ” ì…ë ¥ í¬ê¸° í™•ì¸\n",
    "input_shape = input_details[0]['shape']  # (1, 224, 224, 3)\n",
    "input_size = (input_shape[1], input_shape[2])\n",
    "\n",
    "# ì›¹ìº  ì´ˆê¸°í™”\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # ì´ë¯¸ì§€ í¬ê¸° ë³€í™˜ (ëª¨ë¸ì˜ ê¸°ëŒ€ í¬ê¸°: 224x224)\n",
    "    resized_frame = cv2.resize(frame, input_size)\n",
    "    input_data = np.expand_dims(resized_frame, axis=0).astype(np.float32)  # ì°¨ì› í™•ì¥ ë° float32 ë³€í™˜\n",
    "\n",
    "    # TFLite ëª¨ë¸ ì‹¤í–‰\n",
    "    interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "    interpreter.invoke()\n",
    "    predictions = interpreter.get_tensor(output_details[0]['index'])\n",
    "\n",
    "    # ê°€ì¥ ë†’ì€ í™•ë¥ ì˜ í´ë˜ìŠ¤ ê°€ì ¸ì˜¤ê¸°\n",
    "    predicted_class = np.argmax(predictions)\n",
    "\n",
    "    # í™”ë©´ì— ë¶„ë¥˜ ê²°ê³¼ í‘œì‹œ\n",
    "    cv2.putText(frame, f'Class: {predicted_class}', (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "    cv2.imshow('YOLO Classification (TFLite)', frame)\n",
    "\n",
    "    # 'q' í‚¤ë¥¼ ëˆ„ë¥´ë©´ ì¢…ë£Œ\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
